{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf3735a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f9df6cd2ff1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRouge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import networkx as nx\n",
    "from rouge import Rouge\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "# You may need to download 'punkt' and 'stopwords' from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def similarity_matrix(sentences):\n",
    "    # Create an empty similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentences[i].reshape(1, -1), sentences[j].reshape(1, -1))[0,0]\n",
    "    return sim_mat\n",
    "\n",
    "def vectorize_sentences(sentences):\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    # Convert sparse matrix to dense matrix\n",
    "    vectors = vectorizer.toarray()\n",
    "    return vectors\n",
    "\n",
    "def text_rank(text, n):\n",
    "    # Tokenize the text and remove stopwords\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [' '.join(w for w in word_tokenize(sentence) if w not in stop_words) for sentence in sentences]\n",
    "\n",
    "    # Convert sentences to vectors\n",
    "    vectors = vectorize_sentences(sentences)\n",
    "\n",
    "    # Create a similarity matrix\n",
    "    sim_mat = similarity_matrix(vectors)\n",
    "\n",
    "    # Use the similarity matrix to create a graph\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "\n",
    "    # Apply the PageRank algorithm to the graph\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    # Sort the sentences by score and return the n best\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    best_sentences = [s for score, s in ranked_sentences[:n]]\n",
    "\n",
    "    return ' '.join(best_sentences)\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./dataset/business/037.txt\"\n",
    "\n",
    "# Read the text from the file\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "# Generate the summary\n",
    "summary = text_rank(text, 2)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Specify the file path for saving the summary\n",
    "output_path = \"./res.txt\"\n",
    "\n",
    "# Save the summary to the file\n",
    "with open(output_path, 'w') as file:\n",
    "    file.write(summary)\n",
    "\n",
    "\n",
    "def read_reference_summary(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Specify the file path\n",
    "reference_summary_path = \"./reference res.txt\"\n",
    "\n",
    "# Read the reference summary from the file\n",
    "reference_summary = read_reference_summary(reference_summary_path)\n",
    "\n",
    "def calculate_rouge_scores(hypothesis, reference):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores(summary, reference_summary)\n",
    "print(rouge_scores)\n",
    "\n",
    "def highlight_matches(summary, reference):\n",
    "    summary_words = summary.split()\n",
    "    reference_words = reference.split()\n",
    "    highlighted_summary = ''\n",
    "\n",
    "    for word in summary_words:\n",
    "        if word in reference_words:\n",
    "            highlighted_summary += colored(word, 'green') + ' '\n",
    "        else:\n",
    "            highlighted_summary += word + ' '\n",
    "\n",
    "    print(\"Summary:\")\n",
    "    print(highlighted_summary)\n",
    "    print(\"\\nReference:\")\n",
    "    print(reference)\n",
    "\n",
    "highlight_matches(summary, reference_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16beb2ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-e42dd572397a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-e42dd572397a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install rouge\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126e8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
